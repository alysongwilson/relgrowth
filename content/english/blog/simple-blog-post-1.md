---
author: Alyson Wilson
date: "2020-03-23"
description: Basics of DoD Test and Evaluation
image: images/testandeval.jpg
image_webp: images/testandeval.webp
title: DoD T&E
---

When the Department of Defense is buying a new system or improving an existing one, the system undergoes a series of tests. The goal is to understand how well the system can accomplish its mission, perform in realistic environments, keep its users safe, work reliability, and survive and/or recover from damage. These characteristics are grouped into two categories: *effectiveness* and *suitability*. Effectiveness is how well the system can accomplish its mission when used by trained personnel in its intended environment. Suitability measures how well a system can be used in its intended environment; colloquially, it is said to comprise the ``-ilities,'' such as reliability, availability, compatability, transportability, interoperability, supportability, etc.

The tests themselves also have two different flavors. Some of the tests are *developmental*, which look to see if the system meets requirements. For example, does the system operate when it is cold? Can it be transported using designated equipment? Other tests are *operational*, which assess the system in a mission-realistic environment.

Our interest is in *reliability*. Informally, reliability is the property that the system works when we want to use it. For example, the reliability of an electrical switch might be defined as the probability that the switch works at a particular temperature under a specified load. The International Organization for Standardization (ISO) defines reliability as the ``ability of an item to perform a required function, under given environmental and operating conditions and for a stated period of time.''

When a system is in the early stages of development and prototyping, it typically will not meet the final customer requirements for reliability. To improve reliability, and create *reliability growth*, the tests are used to identify failure modes, which are then analyzed and fixed. As testing continues, improvements are incorporated into the system within designated corrective action periods between test events, with the expectation that reliability grows over time. This process is known as *test-analyze-fix-test*.